diff --git a/xla/mlir/backends/gpu/transforms/BUILD b/xla/mlir/backends/gpu/transforms/BUILD
index da9805c2..30f95f57 100644
--- a/xla/mlir/backends/gpu/transforms/BUILD
+++ b/xla/mlir/backends/gpu/transforms/BUILD
@@ -71,13 +71,20 @@ cc_library(
         "//xla/service/gpu:backend_configs_cc",
         "//xla/service/gpu:gpu_executable",
         "//xla/service/gpu:launch_dimensions",
-        "//xla/service/gpu:nccl_collective_thunks",
         "//xla/service/gpu/runtime3:conditional_thunk",
         "//xla/service/gpu/runtime3:copy_thunk",
         "//xla/service/gpu/runtime3:kernel_thunk",
         "//xla/service/gpu/runtime3:memset_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
+    ] + select({
+        "@local_config_sycl//sycl:using_sycl": [
+            "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
+        ],
+        "//conditions:default": [
+            "//xla/service/gpu:nccl_collective_thunks",
+            "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
+            "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
+        ],
+    }) + [
         "//xla/service/gpu/runtime3:sequential_thunk",
         "//xla/service/gpu/runtime3:while_thunk",
         "//xla/stream_executor:blas",
diff --git a/xla/mlir/backends/gpu/transforms/lmhlo_to_gpu_runtime.cc b/xla/mlir/backends/gpu/transforms/lmhlo_to_gpu_runtime.cc
index 4a8dd0c2..f7cdbce3 100644
--- a/xla/mlir/backends/gpu/transforms/lmhlo_to_gpu_runtime.cc
+++ b/xla/mlir/backends/gpu/transforms/lmhlo_to_gpu_runtime.cc
@@ -43,6 +43,13 @@ limitations under the License.
 #include "xla/mlir/runtime/utils/custom_calls.h"
 #include "xla/mlir_hlo/lhlo/IR/lhlo_ops.h"
 #include "xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.h"
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#else
 #include "xla/service/gpu/nccl_all_to_all_thunk.h"
 #include "xla/service/gpu/nccl_collective_permute_thunk.h"
 #include "xla/service/gpu/nccl_collective_thunk.h"
@@ -50,6 +57,7 @@ limitations under the License.
 #include "xla/service/gpu/nccl_send_thunk.h"
 #include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
 #include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
+#endif
 
 namespace xla {
 namespace gpu {
@@ -579,8 +587,10 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
   static StringRef Target(CollectivePermuteStartOp) {
     return "xla.gpu.collective_permute";
   }
+#if !TENSORFLOW_USE_SYCL
   static StringRef Target(SendOp) { return "xla.gpu.send"; }
   static StringRef Target(RecvOp) { return "xla.gpu.recv"; }
+#endif  // !TENSORFLOW_USE_SYCL
 
   template <typename OpT>
   static std::enable_if_t<
@@ -606,6 +616,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
         op, replica_count, num_partitions);
   }
 
+#if !TENSORFLOW_USE_SYCL
   static NcclCollectiveConfig GetNcclCollectiveConfig(SendOp op,
                                                       int replica_count,
                                                       int num_partitions) {
@@ -619,6 +630,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
     return GetNcclCollectiveConfigForP2POps<NcclRecvThunk, RecvOp>(
         op, replica_count, num_partitions);
   }
+#endif  // !TENSORFLOW_USE_SYCL
 
   template <typename NonCollectivePermuteOp>
   static std::enable_if_t<!is_any<NonCollectivePermuteOp, SendOp, RecvOp>,
@@ -639,6 +651,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
     return success();
   }
 
+#if !TENSORFLOW_USE_SYCL
   // Send/Recv is never degenerate by itself, so returns failure().
   template <typename OpT>
   static std::enable_if_t<is_any<OpT, SendOp, RecvOp>, LogicalResult>
@@ -647,6 +660,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
                          PatternRewriter& rewriter) {
     return failure();
   }
+#endif  // !TENSORFLOW_USE_SYCL
 
   static LogicalResult TryDegenerateToMemCopy(
       CollectivePermuteStartOp op, const NcclCollectiveConfig& config,
@@ -688,6 +702,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
         op, replica_count, num_partitions);
   }
 
+#if !TENSORFLOW_USE_SYCL
   static Status CheckImplementable(SendOp op, int64_t replica_count,
                                    int64_t num_partitions) {
     return NcclSendThunk::CheckImplementable(op, replica_count, num_partitions);
@@ -697,6 +712,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
                                    int64_t num_partitions) {
     return NcclRecvThunk::CheckImplementable(op, replica_count, num_partitions);
   }
+#endif  // !TENSORFLOW_USE_SYCL
 
   static Status CheckImplementable(ReduceScatterStartOp op,
                                    int64_t replica_count,
@@ -767,6 +783,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
     return success();
   }
 
+#if !TENSORFLOW_USE_SYCL
   template <typename OpT>
   static typename std::enable_if_t<is_any<OpT, SendOp, RecvOp>, LogicalResult>
   SetSpecificAttrs(ImplicitLocOpBuilder& b, OpT op, func::CallOp call) {
@@ -784,6 +801,7 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
       OpT) {
     return false;
   }
+#endif  // !TENSORFLOW_USE_SYCL
 
   template <typename OpT>
   static typename std::enable_if_t<!is_any<OpT, SendOp, RecvOp>, bool>
@@ -791,11 +809,13 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
     return op.getIsSync();
   }
 
+#if !TENSORFLOW_USE_SYCL
   template <typename OpT>
   static typename std::enable_if_t<is_any<OpT, SendOp, RecvOp>, bool>
   noParallelCustomCall(OpT) {
     return false;
   }
+#endif  // !TENSORFLOW_USE_SYCL
 
   template <typename OpT>
   static typename std::enable_if_t<!is_any<OpT, SendOp, RecvOp>, bool>
@@ -845,8 +865,10 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {
                                                                      op);
       eraseDoneOp<ReduceScatterStartOp, ReduceScatterDoneOp>(rewriter, op);
       eraseDoneOp<AllToAllStartOp, AllToAllDoneOp>(rewriter, op);
+#if !TENSORFLOW_USE_SYCL
       eraseDoneOp<SendOp, SendDoneOp>(rewriter, op);
       eraseDoneOp<RecvOp, RecvDoneOp>(rewriter, op);
+#endif  // !TENSORFLOW_USE_SYCL
     };
 
     // A given collective op can be degenerate if across all groups formed
@@ -966,8 +988,10 @@ DEFINE_COLLECTIVE_OP_LOWERING(AllReduceStartOp);
 DEFINE_COLLECTIVE_OP_LOWERING(AllToAllStartOp);
 DEFINE_COLLECTIVE_OP_LOWERING(CollectivePermuteStartOp);
 DEFINE_COLLECTIVE_OP_LOWERING(ReduceScatterStartOp);
+#if !TENSORFLOW_USE_SYCL
 DEFINE_COLLECTIVE_OP_LOWERING(SendOp);
 DEFINE_COLLECTIVE_OP_LOWERING(RecvOp);
+#endif  // !TENSORFLOW_USE_SYCL
 
 #undef DEFINE_COLLECTIVE_OP_LOWERING
 
@@ -1019,8 +1043,10 @@ DEFINE_COLLECTIVE_DONE_OP_LOWERING(AllToAllDoneOp, "all_to_all_done");
 DEFINE_COLLECTIVE_DONE_OP_LOWERING(CollectivePermuteDoneOp,
                                    "collective_permute_done");
 DEFINE_COLLECTIVE_DONE_OP_LOWERING(ReduceScatterDoneOp, "reduce_scatter_done");
+#if !TENSORFLOW_USE_SYCL
 DEFINE_COLLECTIVE_DONE_OP_LOWERING(SendDoneOp, "send_done");
 DEFINE_COLLECTIVE_DONE_OP_LOWERING(RecvDoneOp, "recv_done");
+#endif  // !TENSORFLOW_USE_SYCL
 
 #undef DEFINE_COLLECTIVE_DONE_OP_LOWERING
 
@@ -1063,6 +1089,7 @@ class PartitionIdOpLowering : public CollectiveIdOpLowering<PartitionIdOp> {
 // Host<->Device communication ops lowering (Send/Recv).
 //===----------------------------------------------------------------------===//
 
+#if !TENSORFLOW_USE_SYCL
 template <typename OpT, typename Derived>
 class HostSendRecvOpLowering : public OpRewritePattern<OpT> {
  public:
@@ -1133,6 +1160,7 @@ DEFINE_HOST_SENDRECV_OP_LOWERING(SendOp, "xla.gpu.send_host");
 DEFINE_HOST_SENDRECV_OP_LOWERING(SendDoneOp, "xla.gpu.send_done_host");
 DEFINE_HOST_SENDRECV_OP_LOWERING(RecvOp, "xla.gpu.recv_host");
 DEFINE_HOST_SENDRECV_OP_LOWERING(RecvDoneOp, "xla.gpu.recv_done_host");
+#endif  // !TENSORFLOW_USE_SYCL
 
 //===----------------------------------------------------------------------===//
 
@@ -1191,8 +1219,13 @@ void ConvertLmhloToGpuRuntimePass::runOnOperation() {
         std::pair<AllReduceStartOp, AllReduceDoneOp>,
         std::pair<AllToAllStartOp, AllToAllDoneOp>,
         std::pair<CollectivePermuteStartOp, CollectivePermuteDoneOp>,
-        std::pair<ReduceScatterStartOp, ReduceScatterDoneOp>,
-        std::pair<SendOp, SendDoneOp>, std::pair<RecvOp, RecvDoneOp>>(
+        std::pair<ReduceScatterStartOp, ReduceScatterDoneOp>
+#if !TENSORFLOW_USE_SYCL
+        ,
+        std::pair<SendOp, SendDoneOp>,
+        std::pair<RecvOp, RecvDoneOp>
+#endif  // !TENSORFLOW_USE_SYCL
+        >(
         op, collective_uid);
   });
   if (walked.wasInterrupted()) return signalPassFailure();
@@ -1202,14 +1235,22 @@ void ConvertLmhloToGpuRuntimePass::runOnOperation() {
                                                               custom_calls);
   patterns.insert<AllGatherStartOpLowering, AllReduceStartOpLowering,
                   AllToAllStartOpLowering, CollectivePermuteStartOpLowering,
-                  ReduceScatterStartOpLowering, SendOpLowering, RecvOpLowering>(
+                  ReduceScatterStartOpLowering
+#if !TENSORFLOW_USE_SYCL
+                  ,
+                  SendOpLowering,
+                  RecvOpLowering
+#endif  // !TENSORFLOW_USE_SYCL
+                  >(
       ctx, collective_uid, custom_calls);
 
   // Convert lmhlo host<->device point-to-point communication operations to XLA
   // gpu runtime.
+#if !TENSORFLOW_USE_SYCL
   patterns.insert<HostSendOpLowering, HostSendDoneOpLowering,
                   HostRecvOpLowering, HostRecvDoneOpLowering>(ctx,
                                                               custom_calls);
+#endif  // !TENSORFLOW_USE_SYCL
 
   if (failed(applyPatternsAndFoldGreedily(module, std::move(patterns))))
     return signalPassFailure();
@@ -1223,8 +1264,13 @@ void ConvertLmhloToGpuRuntimePass::runOnOperation() {
     RewritePatternSet patterns(ctx);
     patterns.insert<AllGatherDoneOpLowering, AllReduceDoneOpLowering,
                     AllToAllDoneOpLowering, CollectivePermuteDoneOpLowering,
-                    ReduceScatterDoneOpLowering, SendDoneOpLowering,
-                    RecvDoneOpLowering>(ctx, collective_uid, custom_calls);
+                    ReduceScatterDoneOpLowering
+#if !TENSORFLOW_USE_SYCL
+                    ,
+                    SendDoneOpLowering,
+                    RecvDoneOpLowering
+#endif  // !TENSORFLOW_USE_SYCL
+                    >(ctx, collective_uid, custom_calls);
     if (failed(applyPatternsAndFoldGreedily(module, std::move(patterns))))
       return signalPassFailure();
   }
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index e4e7dbfe..78371c3d 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -12,6 +12,7 @@ load(
 )
 load(
     "//xla/stream_executor:build_defs.bzl",
+    "if_cuda_or_rocm",
     "if_gpu_is_configured",
 )
 load(
@@ -1185,7 +1186,6 @@ cc_library(
     visibility = ["//visibility:private"],
     deps = if_gpu_is_configured([
         ":gpu_executable_run_options",
-        ":nccl_collective_thunks",
         ":nccl_clique_key",
         ":nccl_clique",
         ":thunk",
@@ -1198,6 +1198,13 @@ cc_library(
         "//xla/service:collective_ops_utils",
         "//xla/service:global_device_id",
         "//xla/stream_executor",
+    ]) + select({
+        "@local_config_sycl//sycl:using_sycl": [
+            "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
+        ],
+        "//conditions:default": [],
+    }) + if_cuda_or_rocm([
+        ":nccl_collective_thunks",
     ]) + if_cuda_is_configured([
         "@local_config_nccl//:nccl",
     ]),
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index 33a2cc0b..8ff74776 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -79,10 +79,17 @@ cc_library(
         "//xla/service:global_device_id",
         "//xla/service/gpu:gpu_executable_run_options",
         "//xla/service/gpu:nccl_api",
-        "//xla/service/gpu:nccl_collective_thunks",
+    ] + select({
+        "@local_config_sycl//sycl:using_sycl": [
+            "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
+        ],
+        "//conditions:default": [
+            "//xla/service/gpu:nccl_collective_thunks",
+            "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
+            "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
+        ],
+    }) + [
         "//xla/service/gpu:thunk",
-        "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
         "//xla/stream_executor",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/container:flat_hash_set",
diff --git a/xla/service/gpu/runtime/collectives.cc b/xla/service/gpu/runtime/collectives.cc
index e9458053..867ed719 100644
--- a/xla/service/gpu/runtime/collectives.cc
+++ b/xla/service/gpu/runtime/collectives.cc
@@ -32,15 +32,23 @@ limitations under the License.
 #include "xla/service/computation_placer.h"
 #include "xla/service/global_device_id.h"
 #include "xla/service/gpu/gpu_executable_run_options.h"
-#include "xla/service/gpu/nccl_all_to_all_thunk.h"
 #include "xla/service/gpu/nccl_api.h"
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#else
+#include "xla/service/gpu/nccl_all_to_all_thunk.h"
 #include "xla/service/gpu/nccl_collective_permute_thunk.h"
 #include "xla/service/gpu/nccl_collective_thunk.h"
 #include "xla/service/gpu/nccl_recv_thunk.h"
 #include "xla/service/gpu/nccl_send_thunk.h"
-#include "xla/service/gpu/runtime/support.h"
 #include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
 #include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
+#endif
+#include "xla/service/gpu/runtime/support.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/service_executable_run_options.h"
 
@@ -423,6 +431,7 @@ XLA_RUNTIME_DEFINE_CUSTOM_CALL(
 // Send.
 //===----------------------------------------------------------------------===//
 
+#if !TENSORFLOW_USE_SYCL
 static absl::Status P2PSendImpl(const ServiceExecutableRunOptions* run_options,
                                 const DebugOptions* debug_options,
                                 CollectivesSupport* collectives,
@@ -526,6 +535,7 @@ XLA_RUNTIME_DEFINE_CUSTOM_CALL(
         .Attr<absl::Span<const int64_t>>("replica_group_values")
         .Attr<absl::Span<const int64_t>>("source_peers")
         .Attr<absl::Span<const int64_t>>("target_peers"));
+#endif  // !TENSORFLOW_USE_SYCL
 
 //===----------------------------------------------------------------------===//
 // AllGather.
@@ -1011,8 +1021,10 @@ absl::StatusOr<se::Event> AsyncCollectivesSupport::PopEvent(int32_t uid) {
 void RegisterCollectiveCustomCalls(
     runtime::DirectCustomCallRegistry& registry) {
   registry.Register("xla.gpu.collective_permute", CollectivePermute);
+#if !TENSORFLOW_USE_SYCL
   registry.Register("xla.gpu.send", P2PSend);
   registry.Register("xla.gpu.recv", P2PRecv);
+#endif  // !TENSORFLOW_USE_SYCL
   registry.Register("xla.gpu.all_gather", AllGather);
   registry.Register("xla.gpu.all_reduce", AllReduce);
   registry.Register("xla.gpu.all_to_all", AllToAll);
